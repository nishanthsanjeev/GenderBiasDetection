{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simply use (Shift+Enter) to execute each of these code blocks in sequence, to generate vectors for the corpora used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the following libraries. Install them, if not present on your system.\n",
    "import fasttext\n",
    "import os\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to remove html markups, etc\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that generates a list of elements, where each element here is the full path to each of the text files.\n",
    "\n",
    "def getListOfFiles(dirName):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions used for preprocessing textual data\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replaces contractions (it's -> it is)\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the combined text file\n",
    "\n",
    "\n",
    "#Creating a list of the filenames required\n",
    "#all_files = getListOfFiles('/path/to/Corpora')\n",
    "\n",
    "all_files = getListOfFiles('/home/nishanthsanjeev/Harvard/Corpora')\n",
    "\n",
    "\n",
    "# Here, the 'Corpora' file contains the text files that will be opened and combined into the final text file.\n",
    "\n",
    "# Kindly make sure that the only files in this directory are the text files that are being considered, although\n",
    "# they can be stored (separately or not) in any sub-directory, as long as they are all within the 'Corpora' directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the new, combined text file.\n",
    "\n",
    "\n",
    "with open('\\Harvard\\file_combined.txt', 'w') as outfile:\n",
    "    for names in all_files:\n",
    "        print(names)\n",
    "        with open(names,'r') as infile:\n",
    "            outfile.write(infile.read())\n",
    "            \n",
    "        outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the 'combined_data' variable now stores the contents of 'file_combined.txt'\n",
    "\n",
    "combined_data = open('\\Harvard\\file_combined.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_cl = re.sub(r\"\\\\[']\\d*\",\"'\",combined_data)\n",
    "\n",
    "\n",
    "#A common occurence of garbage occurring in the txt files, that I was able to remove:\n",
    "\n",
    "# \"I\\'92ve got a sweet tooth!\"\n",
    "\n",
    "# This regex converts this sentence to:\n",
    "\n",
    "# \"I've got a sweet tooth!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the denoise() function on the data\n",
    "comb_cl_den = denoise_text(comb_cl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing into sentences -> This converts the huge paragraph of text into a list of sentences.\n",
    "\n",
    "sentences = nltk.sent_tokenize(comb_cl_den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing contractions, i.e. it's -> it is, et cetera\n",
    "\n",
    "sentences_mod = []\n",
    "for x in sentences:\n",
    "    x = replace_contractions(x)\n",
    "    sentences_mod.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing each sentence into it's constituent words.\n",
    "\n",
    "# For example,\n",
    "\n",
    "# 'I like ice cream'\n",
    "\n",
    "# becomes a list, with each of its elements being words.\n",
    "\n",
    "# -> ['I','like','ice','cream']\n",
    "\n",
    "\n",
    "\n",
    "words = []\n",
    "for el in sentences_mod:\n",
    "    words.append(nltk.word_tokenize(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if everything went okay:\n",
    "\n",
    "Assume, your input text was:\n",
    "\n",
    "\"I like ice cream. Let's go buy some!\"\n",
    "\n",
    "At this step, the 'words' variable must hold a list of lists.\n",
    "Each of these lists hold tokenized sentences.\n",
    "\n",
    "Therefore, we should get:\n",
    "\n",
    "[['I', 'like', 'ice', 'cream', '.'], ['let', 'us', 'go', 'buy', 'some', '!']]\n",
    "\n",
    "It is important that the data is in the list of lists format, as this is the required input format for Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing non-ascii words\n",
    "non_asc = []\n",
    "for s in words:\n",
    "    temp = remove_non_ascii(s)\n",
    "    non_asc.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to lowercase\n",
    "lower_c = []\n",
    "for s in non_asc:\n",
    "    temp = to_lowercase(s)\n",
    "    lower_c.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctutation\n",
    "punct_g = []\n",
    "for s in lower_c:\n",
    "    temp = remove_punctuation(s)\n",
    "    punct_g.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     Final pre-processing step: Lemmatizing the data. (Converting tokens to their root forms)\n",
    "    \n",
    "#     E.g.: Converts 'running','ran',etc to 'run'\n",
    "\n",
    "\n",
    "train_data = []\n",
    "for f in punct_g:\n",
    "    f = lemmatize_verbs(f)\n",
    "    train_data.append(f)#stop words remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 60\n",
    "window_size = 40\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "ft_model = FastText(train_data,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                    sg=1,\n",
    "                      iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantically_similar_words = {words: [item[0] for item in ft_model.wv.most_similar([words], topn=5)]\n",
    "                  for words in ['kitchen', 'death', 'king', 'queen', 'strong', 'weak','woman','man']}\n",
    "\n",
    "for k,v in semantically_similar_words.items():\n",
    "    print(k+\":\"+str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.similarity(\"annabeth\",\"percy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.wv.save_word2vec_format('FTvectors')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
